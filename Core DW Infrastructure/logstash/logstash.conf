input {
  http {
    port => 5044
    codec => json
  }
}

filter {
  uuid {
    target => "log_id"
  }
  mutate {
    add_field => { "timestamp" => "%{@timestamp}" }
  }
  date {
    match => [ "timestamp", "ISO8601" ]
    target => "@timestamp"
  }

  # Unwrap the Records array for MinIO logs
  if [Records] {
    split {
      field => "Records"
    }
    mutate {
      rename => { "[Records][eventName]" => "event_type" }
      rename => { "[Records][s3][bucket][name]" => "bucket_name" }
      rename => { "[Records][s3][object][key]" => "object_key" }
      rename => { "[Records][s3][object][size]" => "object_size" }
      rename => { "[Records][s3][object][eTag]" => "object_etag" }
      rename => { "[Records][s3][object][contentType]" => "content_type" }
      rename => { "[Records][eventTime]" => "timestamp" }
      rename => { "[Records][requestParameters][sourceIPAddress]" => "source_ip" }
      rename => { "[Records][userIdentity][principalId]" => "user_id" }
      rename => { "[Records][eventSource]" => "event_source" }
    }
    mutate {
      add_field => { "service_endpoint" => "%{[Records][responseElements][x-minio-origin-endpoint]}" }
      add_field => { "custom_metadata" => null }
    }
    mutate {
      convert => { "object_size" => "integer" }
    }
  }
}

output {
  # Directly pass logs from file upload service to Elasticsearch
  if [source] == "upload_service" {
    elasticsearch {
      hosts => ["http://dp-elasticsearch:9200"]
      index => "fileupload-%{+YYYY.MM.dd}"
    }
  }

  # Process and output MinIO logs
  if [event_source] == "minio:s3" {
    # Provenance Store and Elasticsearch
    if [event_type] =~ /^s3:ObjectCreated:/ or [event_type] =~ /^s3:ObjectRemoved:/ or [event_type] =~ /^s3:Lifecycle:/ or [event_type] =~ /^s3:Replication:/ {
      jdbc {
        driver_jar_path => "/usr/share/logstash/logstash-core/lib/jars/postgresql-42.2.18.jar"
        driver_class => "org.postgresql.Driver"
        connection_string => "jdbc:postgresql://${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}"
        username => "${POSTGRES_USER}"
        password => "${POSTGRES_PASSWORD}"
        statement => [
          "INSERT INTO provenance (event_source, event_type, user_id, source_ip, bucket_name, object_key, object_size, object_etag, content_type, service_endpoint, custom_metadata) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CAST(? AS JSONB))",
          "event_source", "event_type", "user_id", "source_ip", "bucket_name", "object_key", "object_size", "object_etag", "content_type", "service_endpoint", "custom_metadata"
        ]
      }
      elasticsearch {
        hosts => ["http://dp-elasticsearch:9200"]
        index => "minio-%{+YYYY.MM.dd}"
      }
    } else if [event_type] =~ /^s3:ObjectAccessed:/ or [event_type] =~ /^s3:Error:/ or [event_type] =~ /^s3:Audit:/ or [event_type] =~ /^s3:BucketPolicy:/ or [event_type] =~ /^s3:BucketNotification:/  or [event_type] in ["s3:BucketCreated", "s3:BucketRemoved"]{
      elasticsearch {
        hosts => ["http://dp-elasticsearch:9200"]
        index => "minio-%{+YYYY.MM.dd}"
      }
    } else if [event_type] =~ /^s3:ObjectRestore:/ or [event_type] =~ /^s3:ObjectTransition:/ {
      jdbc {
        driver_jar_path => "/usr/share/logstash/logstash-core/lib/jars/postgresql-42.2.18.jar"
        driver_class => "org.postgresql.Driver"
        connection_string => "jdbc:postgresql://${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}"
        username => "${POSTGRES_USER}"
        password => "${POSTGRES_PASSWORD}"
        statement => [
          "INSERT INTO provenance (event_source, event_type, user_id, source_ip, bucket_name, object_key, object_size, object_etag, content_type, service_endpoint, custom_metadata) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CAST(? AS JSONB))",
          "event_source", "event_type", "user_id", "source_ip", "bucket_name", "object_key", "object_size", "object_etag", "content_type", "service_endpoint", "custom_metadata"
        ]
      }
    }
  }

  stdout { codec => rubydebug }
}